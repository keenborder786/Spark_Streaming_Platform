
FROM continuumio/miniconda3:latest

WORKDIR /opt

## Updating the current conda environment with desired packages
COPY environment.yml .
RUN conda update -n base conda && \
    conda env update --name base -f environment.yml

# Install base utilities
RUN apt-get update && \
    apt-get install -y build-essential  && \
    apt-get install -yq curl wget jq vim && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

## Install java and spark
RUN apt update && \
    apt install -y default-jdk && \
    wget https://downloads.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz && \
    tar -xzf spark-3.3.1-bin-hadoop3.tgz && \
    mv spark-3.3.1-bin-hadoop3 /opt/spark

ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin
ENV SPARK_NO_DAEMONIZE=true