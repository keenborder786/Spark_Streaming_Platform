FROM quay.io/oknah/sparkbase:1.0

ENV MASTER_HOST_NAME=''
ENV DRIVER_HOST = ''
ENV DRIVER_IP=''
ENV DRIVER_PORT=''
ENV DRIVER_BLOCK_MANAGER_PORT=''
ENV EXECUTOR_CORES=''
ENV EXECUTOR_MEMORY=''
ENV MIN_EXECUTOR_NO=''
ENV MAX_EXECUTOR_NO=''
ENV SHUFFLE_PARTITION=''
## Setting up my code
COPY streaming/ /opt/poc_kafka_delta/streaming/
COPY main.py /opt/poc_kafka_delta
COPY .env.schema /opt/poc_kafka_delta
COPY simulate_kafka.py /opt/poc_kafka_delta
COPY spark-defaults.conf /opt/spark/conf

## Execute the final shell script to run our job
CMD $SPARK_HOME/bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,io.delta:delta-core_2.12:2.1.1,com.amazonaws:aws-java-sdk:1.12.364,org.apache.hadoop:hadoop-aws:3.3.4,org.apache.hadoop:hadoop-common:3.3.4 \
    --conf spark.master=$MASTER_HOST_NAME \
    --conf spark.driver.host=$DRIVER_HOST \
    --conf spark.driver.bindAddress=$DRIVER_IP \
    --conf spark.driver.port=$DRIVER_PORT \
    --conf spark.executor.cores=$EXECUTOR_CORES \
    --conf spark.executor.memory=$EXECUTOR_MEMORY \
    --conf spark.dynamicAllocation.minExecutors=$MIN_EXECUTOR_NO \
    --conf spark.dynamicAllocation.maxExecutors=$MAX_EXECUTOR_NO \
    --conf spark.driver.blockManager.port=$DRIVER_BLOCK_MANAGER_PORT \
    --conf spark.sql.shuffle.partitions=$SHUFFLE_PARTITION \
    --conf spark.pyspark.python=/opt/conda/envs/spark_streaming/bin/python \
    /opt/poc_kafka_delta/main.py